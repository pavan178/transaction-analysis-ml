# -*- coding: utf-8 -*-
"""Fraud_Detection_AI_Crypto (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1op95Hqsl6IpVDuAGWHXdx7Qly6xzd02J
"""

import pandas as pd
import numpy as np

df = pd.read_csv(r'C:\Users\hp\Downloads\archive (2)\transaction_dataset.csv',index_col=0)

df = df.drop(df.columns[:2], axis=1)

df.dtypes

category = df.select_dtypes('O').columns.astype('category')
df[category]

category

df.fillna('', inplace=True)



#df[category].fillna('', inplace=True)
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
# Tokenize the categorical columns
mlb = MultiLabelBinarizer()
tokenized = mlb.fit_transform(df[category].values)

# Create a new DataFrame with the tokenized data
tokenized_df = pd.DataFrame(tokenized, columns=mlb.classes_, index=df[category].index)

print(tokenized_df)

import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Sample data
# Assuming 'tokenized_df' is your DataFrame after tokenization

# Standardize the data
scaler = StandardScaler()
tokenized_scaled = scaler.fit_transform(tokenized_df)

# Apply PCA
pca_df = pd.DataFrame(data=tokenized_pca)
pca_df.columns = pca_df.columns.astype(str)

# Print the original column names corresponding to the PCA components
original_column_names = tokenized_df.columns
for i, col in enumerate(pca_df.columns):
    print(f"PCA Component {i} corresponds to original column: {original_column_names[i]}")

print("Original shape:", tokenized_df.shape)
print("Reduced shape:", pca_df.shape)

pca_df

concatenated_df = pd.concat([df, pca_df], axis=1)

concatenated_df

concatenated_df.fillna(np.mean,inplace=True)

import matplotlib.pyplot as plt
import seaborn as sns

# Display missing values
plt.figure(figsize=(10, 6))
sns.heatmap(concatenated_df.isna(), cmap='viridis', cbar=False)
plt.title('Missing Values')
plt.xlabel('Columns')
plt.ylabel('Rows')
plt.show()

concatenated_df.columns=concatenated_df.columns.str.replace(' ', '_')

concatenated_df.columns

concatenated_df

concatenated_df

# Copy the DataFrame to avoid modifying the original
normalized = concatenated_df.copy()


# Columns to be normalized (excluding 'FLAG')

normalized[columns_to_normalize]=normalized[columns_to_normalize].replace('', np.nan, regex=True)



from sklearn.preprocessing import StandardScaler

# Columns to normalize (excluding 'FLAG')
columns_to_normalize = concatenated_df.columns.difference(['FLAG'])

# Select only numeric columns for normalization
numeric_df = concatenated_df[columns_to_normalize].select_dtypes(include=[float, int])

# Initialize StandardScaler
scaler = StandardScaler()

# Fit and transform the selected numeric columns
normalized_numeric_df = pd.DataFrame(scaler.fit_transform(numeric_df), columns=numeric_df.columns, index=numeric_df.index)

# Combine the normalized numeric columns with the non-numeric columns
normalized_concatenated_df = pd.concat([normalized_numeric_df, concatenated_df.drop(columns=columns_to_normalize)], axis=1)

normalized_concatenated_df

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming normalized is your DataFrame and FLAG is the target column
# Calculate the correlation matrix
corr_matrix = normalized_concatenated_df.corr()

# Visualize the correlation matrix as a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

# Identify the features that have the highest absolute correlation values with the target column
target_correlation = corr_matrix['FLAG'].abs().sort_values(ascending=False)

# Select the top features based on their correlation values
top_features = target_correlation[1:11]  # Excluding the target column itself

print("Top Features:")
print(top_features)

column_names = top_features.index.tolist()

print(column_names)

X = normalized_concatenated_df[column_names]
y = normalized_concatenated_df['FLAG'].astype(int)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns

class BinaryClassifier:
    def __init__(self, model=None):
        if model is None:
            self.model = RandomForestClassifier()  # You can change the model here
        else:
            self.model = model

    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)

    def evaluate(self, X_test, y_test):
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]
        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
        roc_auc = auc(fpr, tpr)

        plt.figure()
        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic (ROC)')
        plt.legend(loc="lower right")
        plt.show()

        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print(f'Accuracy: {accuracy}')

        cm = confusion_matrix(y_test, y_pred)
        plt.figure()
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.xlabel('Predicted labels')
        plt.ylabel('True labels')
        plt.title('Confusion Matrix')
        plt.show()

        return roc_auc, accuracy

    def cross_validate(self, X, y, n_splits=5):
        skf = StratifiedKFold(n_splits=n_splits, shuffle=True)
        roc_aucs = []
        accuracies = []
        for train_index, test_index in skf.split(X, y):
            X_train, X_test = X.iloc[train_index], X.iloc[test_index]
            y_train, y_test = y.iloc[train_index], y.iloc[test_index]

            self.train(X_train, y_train)
            roc_auc, accuracy = self.evaluate(X_test, y_test)
            roc_aucs.append(roc_auc)
            accuracies.append(accuracy)

        mean_roc_auc = np.mean(roc_aucs)
        mean_accuracy = np.mean(accuracies)
        print(f'Mean ROC AUC: {mean_roc_auc}')
        print(f'Mean Accuracy: {mean_accuracy}')
        return mean_roc_auc, mean_accuracy



# Split data into train, validation, and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Initialize and train the model
model = BinaryClassifier()
model.train(X_train, y_train)

# Evaluate on validation set
roc_auc_val, accuracy_val = model.evaluate(X_val, y_val)
print(f'ROC AUC on validation set: {roc_auc_val}')
print(f'Accuracy on validation set: {accuracy_val}')

# Perform k-fold cross-validation
roc_auc_mean, accuracy_mean = model.cross_validate(X_train, y_train, n_splits=5)
print(f'Mean ROC AUC: {roc_auc_mean}')
print(f'Mean Accuracy: {accuracy_mean}')

# Evaluate on test set
roc_auc_test, accuracy_test = model.evaluate(X_test, y_test)
print(f'ROC AUC on test set: {roc_auc_test}')
print(f'Accuracy on test set: {accuracy_test}')